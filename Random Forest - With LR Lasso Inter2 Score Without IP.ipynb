{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to concatenate all the created features and prepare the modeling and test dataset for Xgboost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(12, 5)},\n",
    "        font_scale=1.5,\n",
    "        style=\"whitegrid\")\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id, col\n",
    "import pyspark.sql.functions as func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder = \"/user/a008495/ADT/\"\n",
    "datapath = project_folder + \"data/\"\n",
    "plotpath = project_folder + \"graphs/\"\n",
    "mungepath = project_folder + \"munge/\"\n",
    "configpath = project_folder + \"config/\"\n",
    "diagnostic = project_folder + \"diagnostic/\"\n",
    "modelpath = project_folder + \"model/\"\n",
    "output = project_folder + \"output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 184903890 observations in training set.\n"
     ]
    }
   ],
   "source": [
    "train = spark.read.csv(os.path.join(datapath,\"train.csv\"), header=True)\n",
    "print('Found %d observations in training set.' %train.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+---+-------+-------------------+---------------+-------------+\n",
      "|   ip|app|device| os|channel|         click_time|attributed_time|is_attributed|\n",
      "+-----+---+------+---+-------+-------------------+---------------+-------------+\n",
      "|83230|  3|     1| 13|    379|2017-11-06 14:32:21|           null|            0|\n",
      "|17357|  3|     1| 19|    379|2017-11-06 14:33:34|           null|            0|\n",
      "|35810|  3|     1| 13|    379|2017-11-06 14:34:12|           null|            0|\n",
      "+-----+---+------+---+-------+-------------------+---------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ip: string (nullable = true)\n",
      " |-- app: string (nullable = true)\n",
      " |-- device: string (nullable = true)\n",
      " |-- os: string (nullable = true)\n",
      " |-- channel: string (nullable = true)\n",
      " |-- click_time: string (nullable = true)\n",
      " |-- attributed_time: string (nullable = true)\n",
      " |-- is_attributed: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18790469 observations in test set.\n"
     ]
    }
   ],
   "source": [
    "test = spark.read.csv(os.path.join(datapath,\"test.csv\"), header=True)\n",
    "print('Found %d observations in test set.' %test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datetime features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# day function\n",
    "def get_day(date):\n",
    "    \"\"\"\n",
    "    Returns the hour based on a string date\n",
    "        \n",
    "    Args:\n",
    "        date (String): A String containing the click datetime\n",
    "\n",
    "    Returns:\n",
    "        day: A String containing the day of click (\"06\", \"07\", ..\"10\")\n",
    "    \"\"\"\n",
    "    \n",
    "    return date[8:10]\n",
    "\n",
    "# hour function\n",
    "def get_hour(date):\n",
    "    \"\"\"\n",
    "    Returns the hour based on a string date\n",
    "        \n",
    "    Args:\n",
    "        date (String): A String containing the click datetime\n",
    "\n",
    "    Returns:\n",
    "        hour: A String containing the hour interval (\"01\", \"02\", ..\"23\")\n",
    "    \"\"\"\n",
    "    \n",
    "    return date[11:13]\n",
    "\n",
    "# minute function\n",
    "def get_minute(date):\n",
    "    \"\"\"\n",
    "    Returns the minute based on a string date\n",
    "        \n",
    "    Args:\n",
    "        date (String): A String containing the click datetime\n",
    "\n",
    "    Returns:\n",
    "        hour: A String containing the minute interval (\"01\", \"02\", ..\"59\")\n",
    "    \"\"\"\n",
    "    \n",
    "    return date[14:16]\n",
    "\n",
    "# minute function\n",
    "def get_second(date):\n",
    "    \"\"\"\n",
    "    Returns the second based on a string date\n",
    "        \n",
    "    Args:\n",
    "        date (String): A String containing the click datetime\n",
    "\n",
    "    Returns:\n",
    "        hour: A String containing the minute interval (\"01\", \"02\", ..\"59\")\n",
    "    \"\"\"\n",
    "    \n",
    "    return date[17:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get datetime features\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "day_udf = udf(get_day, StringType())\n",
    "hour_udf = udf(get_hour, StringType())\n",
    "minute_udf = udf(get_minute, StringType())\n",
    "second_udf = udf(get_second, StringType())\n",
    "\n",
    "train = train.withColumn('day', day_udf(train.click_time))\n",
    "train = train.withColumn('hour', hour_udf(train.click_time))\n",
    "train = train.withColumn('minute', minute_udf(train.click_time))\n",
    "train = train.withColumn('second', second_udf(train.click_time))\n",
    "\n",
    "test = test.withColumn('day', day_udf(test.click_time))\n",
    "test = test.withColumn('hour', hour_udf(test.click_time))\n",
    "test = test.withColumn('minute', minute_udf(test.click_time))\n",
    "test = test.withColumn('second', second_udf(test.click_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+---+-------+-------------------+---------------+-------------+---+----+------+------+\n",
      "|   ip|app|device| os|channel|         click_time|attributed_time|is_attributed|day|hour|minute|second|\n",
      "+-----+---+------+---+-------+-------------------+---------------+-------------+---+----+------+------+\n",
      "|83230|  3|     1| 13|    379|2017-11-06 14:32:21|           null|            0| 06|  14|    32|    21|\n",
      "|17357|  3|     1| 19|    379|2017-11-06 14:33:34|           null|            0| 06|  14|    33|    34|\n",
      "|35810|  3|     1| 13|    379|2017-11-06 14:34:12|           null|            0| 06|  14|    34|    12|\n",
      "+-----+---+------+---+-------+-------------------+---------------+-------------+---+----+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression score as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 92458650 observations in f_lr_hash_modeling2.\n",
      "Found 18790469 observations in f_lr_hash_test.\n"
     ]
    }
   ],
   "source": [
    "f_lr_hash_modeling2 = spark.read.csv(os.path.join(mungepath, \"logisitc_regression/lasso/f_lr_hash_inter2_2p18_noip_modeling2_split\"), header=True)\n",
    "f_lr_hash_test = spark.read.csv(os.path.join(mungepath, \"logisitc_regression/lasso/f_lr_hash_inter2_2p18_noip_test\"), header=True)\n",
    "print('Found %d observations in f_lr_hash_modeling2.' %f_lr_hash_modeling2.count())\n",
    "print('Found %d observations in f_lr_hash_test.' %f_lr_hash_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert id to long instead of string\n",
    "f_lr_hash_modeling2 = f_lr_hash_modeling2.withColumn('id', col('id').cast('long'))\n",
    "f_lr_hash_test = f_lr_hash_test.withColumn('id', col('id').cast('long'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------------+\n",
      "|          id|f_lr_hash_inter2_2p18_noip|\n",
      "+------------+--------------------------+\n",
      "|403727430145|                    7.0E-5|\n",
      "|403727812258|                    9.1E-5|\n",
      "|403727812631|                    9.1E-5|\n",
      "+------------+--------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_lr_hash_modeling2.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features built for direct concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 184903890 observations in train.\n",
      "Found 18790469 observations in test.\n"
     ]
    }
   ],
   "source": [
    "f_close_clicks_train = spark.read.csv(os.path.join(mungepath, \"f_close_clicks_train\"), header=True)\n",
    "f_close_clicks_test = spark.read.csv(os.path.join(mungepath, \"f_close_clicks_test\"), header=True)\n",
    "print('Found %d observations in train.' %f_close_clicks_train.count())\n",
    "print('Found %d observations in test.' %f_close_clicks_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+\n",
      "| id|lapse_prev_click|lapse_next_click|\n",
      "+---+----------------+----------------+\n",
      "|  0|              -1|            3600|\n",
      "|  1|              -1|            3600|\n",
      "|  2|              -1|            3600|\n",
      "+---+----------------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_close_clicks_train.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 184903890 observations in train.\n",
      "Found 18790469 observations in test.\n"
     ]
    }
   ],
   "source": [
    "f_close_clicks_app_train = spark.read.csv(os.path.join(mungepath, \"f_close_clicks_app_train\"), header=True)\n",
    "f_close_clicks_app_test = spark.read.csv(os.path.join(mungepath, \"f_close_clicks_app_test\"), header=True)\n",
    "print('Found %d observations in train.' %f_close_clicks_app_train.count())\n",
    "print('Found %d observations in test.' %f_close_clicks_app_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|lapse_prev_click_app|lapse_next_click_app|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|                  -1|                3600|\n",
      "|  1|                  -1|                3600|\n",
      "|  2|                  -1|                3600|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_close_clicks_app_train.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 184903890 observations in train.\n",
      "Found 18790469 observations in test.\n"
     ]
    }
   ],
   "source": [
    "f_close_clicks_device_train = spark.read.csv(os.path.join(mungepath, \"f_close_clicks_device_train\"), header=True)\n",
    "f_close_clicks_device_test = spark.read.csv(os.path.join(mungepath, \"f_close_clicks_device_test\"), header=True)\n",
    "print('Found %d observations in train.' %f_close_clicks_device_train.count())\n",
    "print('Found %d observations in test.' %f_close_clicks_device_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+-----------------------+\n",
      "| id|lapse_prev_click_device|lapse_next_click_device|\n",
      "+---+-----------------------+-----------------------+\n",
      "|  0|                     -1|                   3600|\n",
      "|  1|                     -1|                   3600|\n",
      "|  2|                     -1|                   3600|\n",
      "+---+-----------------------+-----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_close_clicks_device_train.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 184903890 observations in train.\n",
      "Found 18790469 observations in test.\n"
     ]
    }
   ],
   "source": [
    "f_close_clicks_os_train = spark.read.csv(os.path.join(mungepath, \"f_close_clicks_os_train\"), header=True)\n",
    "f_close_clicks_os_test = spark.read.csv(os.path.join(mungepath, \"f_close_clicks_os_test\"), header=True)\n",
    "print('Found %d observations in train.' %f_close_clicks_os_train.count())\n",
    "print('Found %d observations in test.' %f_close_clicks_os_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+\n",
      "| id|lapse_prev_click_os|lapse_next_click_os|\n",
      "+---+-------------------+-------------------+\n",
      "|  0|                 -1|               3600|\n",
      "|  1|                 -1|               3600|\n",
      "|  2|                 -1|               3600|\n",
      "+---+-------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_close_clicks_os_train.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features built for join operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4633405 observations.\n"
     ]
    }
   ],
   "source": [
    "f_counts_hour_ip = spark.read.csv(os.path.join(mungepath, \"f_counts_hour_ip\"), header=True)\n",
    "print('Found %d observations.' %f_counts_hour_ip.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----+-----------------+---------------+------------------+-------------+-------------------+------------------+------------------+\n",
      "|   ip|day|hour|ip_hour_nb_clicks|ip_hour_nb_apps|ip_hour_nb_devices|ip_hour_nb_os|ip_hour_nb_channels|ip_hour_std_minute|ip_hour_avg_minute|\n",
      "+-----+---+----+-----------------+---------------+------------------+-------------+-------------------+------------------+------------------+\n",
      "|75595| 06|  22|              330|             22|                 3|           29|                 56|             17.05|              36.4|\n",
      "+-----+---+----+-----------------+---------------+------------------+-------------+-------------------+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_counts_hour_ip.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 57437935 observations.\n"
     ]
    }
   ],
   "source": [
    "f_counts_minute_ip = spark.read.csv(os.path.join(mungepath, \"f_counts_minute_ip\"), header=True)\n",
    "print('Found %d observations.' %f_counts_minute_ip.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----+------+-------------------+-----------------+--------------------+---------------+---------------------+--------------------+--------------------+\n",
      "|    ip|day|hour|minute|ip_minute_nb_clicks|ip_minute_nb_apps|ip_minute_nb_devices|ip_minute_nb_os|ip_minute_nb_channels|ip_minute_std_second|ip_minute_avg_second|\n",
      "+------+---+----+------+-------------------+-----------------+--------------------+---------------+---------------------+--------------------+--------------------+\n",
      "|100009| 07|  03|    31|                  6|                5|                   1|              1|                    6|               11.23|               23.83|\n",
      "+------+---+----+------+-------------------+-----------------+--------------------+---------------+---------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_counts_minute_ip.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 364779 observations.\n"
     ]
    }
   ],
   "source": [
    "f_counts_week_ip = spark.read.csv(os.path.join(mungepath, \"f_counts_week_ip\"), header=True)\n",
    "print('Found %d observations.' %f_counts_week_ip.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+----------+-------------+--------+--------------+-----------------+-----------+-------------+-----------+-------------+\n",
      "|    ip|ip_nb_clicks|ip_nb_apps|ip_nb_devices|ip_nb_os|ip_nb_channels|ip_std_click_time|ip_std_hour|ip_std_minute|ip_avg_hour|ip_avg_minute|\n",
      "+------+------------+----------+-------------+--------+--------------+-----------------+-----------+-------------+-----------+-------------+\n",
      "|121867|        1915|        41|            4|      38|           103|         118201.0|       5.55|        16.63|       9.22|        29.31|\n",
      "+------+------------+----------+-------------+--------+--------------+-----------------+-----------+-------------+-----------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_counts_week_ip.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 769 observations.\n"
     ]
    }
   ],
   "source": [
    "f_counts_week_app = spark.read.csv(os.path.join(mungepath, \"f_counts_week_app\"), header=True)\n",
    "print('Found %d observations.' %f_counts_week_app.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+----------+--------------+---------+---------------+------------------+------------+--------------+------------+--------------+\n",
      "|app|app_nb_clicks|app_nb_ips|app_nb_devices|app_nb_os|app_nb_channels|app_std_click_time|app_std_hour|app_std_minute|app_avg_hour|app_avg_minute|\n",
      "+---+-------------+----------+--------------+---------+---------------+------------------+------------+--------------+------------+--------------+\n",
      "|296|          497|       392|             5|       10|              1|           71876.0|        4.79|          16.3|        8.64|         30.16|\n",
      "+---+-------------+----------+--------------+---------+---------------+------------------+------------+--------------+------------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_counts_week_app.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4228 observations.\n"
     ]
    }
   ],
   "source": [
    "f_counts_week_device = spark.read.csv(os.path.join(mungepath, \"f_counts_week_device\"), header=True)\n",
    "print('Found %d observations.' %f_counts_week_device.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+--------------+-------------+------------+------------------+\n",
      "|device|device_nb_clicks|device_nb_apps|device_nb_ips|device_nb_os|device_nb_channels|\n",
      "+------+----------------+--------------+-------------+------------+------------------+\n",
      "|   296|             143|             2|           91|           1|                 7|\n",
      "+------+----------------+--------------+-------------+------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_counts_week_device.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 957 observations.\n"
     ]
    }
   ],
   "source": [
    "f_counts_week_os = spark.read.csv(os.path.join(mungepath, \"f_counts_week_os\"), header=True)\n",
    "print('Found %d observations.' %f_counts_week_os.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+----------+-------------+---------+--------------+\n",
      "| os|os_nb_clicks|os_nb_apps|os_nb_devices|os_nb_ips|os_nb_channels|\n",
      "+---+------------+----------+-------------+---------+--------------+\n",
      "|675|           7|         3|            1|        5|             3|\n",
      "+---+------------+----------+-------------+---------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_counts_week_os.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 204 observations.\n"
     ]
    }
   ],
   "source": [
    "f_counts_week_channel = spark.read.csv(os.path.join(mungepath, \"f_counts_week_channel\"), header=True)\n",
    "print('Found %d observations.' %f_counts_week_channel.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+---------------+------------------+-------------+--------------+\n",
      "|channel|channel_nb_clicks|channel_nb_apps|channel_nb_devices|channel_nb_os|channel_nb_ips|\n",
      "+-------+-----------------+---------------+------------------+-------------+--------------+\n",
      "|    467|           373151|              2|                 6|          114|         54297|\n",
      "+-------+-----------------+---------------+------------------+-------------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_counts_week_channel.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4533337 observations.\n"
     ]
    }
   ],
   "source": [
    "f_ip_app_clicks = spark.read.csv(os.path.join(mungepath, \"f_ip_app_clicks\"), header=True)\n",
    "print('Found %d observations.' %f_ip_app_clicks.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+-------------+---------------------+---------------+-----------------+---------------+-----------------+\n",
      "|    ip|app|ip_app_clicks|ip_app_std_click_time|ip_app_std_hour|ip_app_std_minute|ip_app_avg_hour|ip_app_avg_minute|\n",
      "+------+---+-------------+---------------------+---------------+-----------------+---------------+-----------------+\n",
      "|105388| 64|           45|              60882.0|           6.34|            17.09|           9.29|            32.44|\n",
      "+------+---+-------------+---------------------+---------------+-----------------+---------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_ip_app_clicks.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 911450 observations.\n"
     ]
    }
   ],
   "source": [
    "f_ip_device_clicks = spark.read.csv(os.path.join(mungepath, \"f_ip_device_clicks\"), header=True)\n",
    "print('Found %d observations.' %f_ip_device_clicks.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------------+------------------------+------------------+--------------------+------------------+--------------------+\n",
      "|   ip|device|ip_device_clicks|ip_device_std_click_time|ip_device_std_hour|ip_device_std_minute|ip_device_avg_hour|ip_device_avg_minute|\n",
      "+-----+------+----------------+------------------------+------------------+--------------------+------------------+--------------------+\n",
      "|84942|     1|           16261|                103078.0|              6.11|               17.22|             11.06|               28.86|\n",
      "+-----+------+----------------+------------------------+------------------+--------------------+------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_ip_device_clicks.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3578758 observations.\n"
     ]
    }
   ],
   "source": [
    "f_ip_os_clicks = spark.read.csv(os.path.join(mungepath, \"f_ip_os_clicks\"), header=True)\n",
    "print('Found %d observations.' %f_ip_os_clicks.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------------+--------------------+--------------+----------------+--------------+----------------+\n",
      "|   ip| os|ip_os_clicks|ip_os_std_click_time|ip_os_std_hour|ip_os_std_minute|ip_os_avg_hour|ip_os_avg_minute|\n",
      "+-----+---+------------+--------------------+--------------+----------------+--------------+----------------+\n",
      "|59395| 19|       23982|             99425.0|          6.12|           17.49|         10.77|           29.35|\n",
      "+-----+---+------------+--------------------+--------------+----------------+--------------+----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_ip_os_clicks.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 46317015 observations.\n"
     ]
    }
   ],
   "source": [
    "f_ip_app_hour_clicks = spark.read.csv(os.path.join(mungepath, \"f_ip_app_hour_clicks\"), header=True)\n",
    "print('Found %d observations.' %f_ip_app_hour_clicks.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+----+------------------+----------------------+----------------------+\n",
      "|    ip|app|day|hour|ip_app_hour_clicks|ip_app_hour_std_minute|ip_app_hour_avg_minute|\n",
      "+------+---+---+----+------------------+----------------------+----------------------+\n",
      "|172522|  3| 06|  15|                 1|                   0.0|                  58.0|\n",
      "+------+---+---+----+------------------+----------------------+----------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_ip_app_hour_clicks.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6259633 observations.\n"
     ]
    }
   ],
   "source": [
    "f_ip_device_hour_clicks = spark.read.csv(os.path.join(mungepath, \"f_ip_device_hour_clicks\"), header=True)\n",
    "print('Found %d observations.' %f_ip_device_hour_clicks.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+----+---------------------+-------------------------+-------------------------+\n",
      "|    ip|device|day|hour|ip_device_hour_clicks|ip_device_hour_std_minute|ip_device_hour_avg_minute|\n",
      "+------+------+---+----+---------------------+-------------------------+-------------------------+\n",
      "|111189|     1| 06|  17|                   56|                    13.64|                    28.63|\n",
      "+------+------+---+----+---------------------+-------------------------+-------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_ip_device_hour_clicks.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26491410 observations.\n"
     ]
    }
   ],
   "source": [
    "f_ip_os_hour_clicks = spark.read.csv(os.path.join(mungepath, \"f_ip_os_hour_clicks\"), header=True)\n",
    "print('Found %d observations.' %f_ip_os_hour_clicks.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------------+--------------------+--------------+----------------+--------------+----------------+\n",
      "|   ip| os|ip_os_clicks|ip_os_std_click_time|ip_os_std_hour|ip_os_std_minute|ip_os_avg_hour|ip_os_avg_minute|\n",
      "+-----+---+------------+--------------------+--------------+----------------+--------------+----------------+\n",
      "|59395| 19|       23982|             99425.0|          6.12|           17.49|         10.77|           29.35|\n",
      "+-----+---+------------+--------------------+--------------+----------------+--------------+----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_ip_os_clicks.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 175594102 observations.\n"
     ]
    }
   ],
   "source": [
    "f_ip_app_minute_clicks = spark.read.csv(os.path.join(mungepath, \"f_ip_app_minute_clicks\"), header=True)\n",
    "print('Found %d observations.' %f_ip_app_minute_clicks.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+----+------+--------------------+------------------------+------------------------+\n",
      "|   ip|app|day|hour|minute|ip_app_minute_clicks|ip_app_minute_std_second|ip_app_minute_avg_second|\n",
      "+-----+---+---+----+------+--------------------+------------------------+------------------------+\n",
      "|29045| 64| 06|  14|    54|                   1|                     0.0|                    59.0|\n",
      "+-----+---+---+----+------+--------------------+------------------------+------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_ip_app_minute_clicks.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60890812 observations.\n"
     ]
    }
   ],
   "source": [
    "f_ip_device_minute_clicks = spark.read.csv(os.path.join(mungepath, \"f_ip_device_minute_clicks\"), header=True)\n",
    "print('Found %d observations.' %f_ip_device_minute_clicks.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+----+------+-----------------------+---------------------------+---------------------------+\n",
      "|   ip|device|day|hour|minute|ip_device_minute_clicks|ip_device_minute_std_second|ip_device_minute_avg_second|\n",
      "+-----+------+---+----+------+-----------------------+---------------------------+---------------------------+\n",
      "|80447|     1| 06|  14|    40|                      1|                        0.0|                       51.0|\n",
      "+-----+------+---+----+------+-----------------------+---------------------------+---------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_ip_device_minute_clicks.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 85228220 observations.\n"
     ]
    }
   ],
   "source": [
    "f_ip_os_minute_clicks = spark.read.csv(os.path.join(mungepath, \"f_ip_os_minute_clicks\"), header=True)\n",
    "print('Found %d observations.' %f_ip_os_minute_clicks.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+----+------+-------------------+-----------------------+-----------------------+\n",
      "|  ip| os|day|hour|minute|ip_os_minute_clicks|ip_os_minute_std_second|ip_os_minute_avg_second|\n",
      "+----+---+---+----+------+-------------------+-----------------------+-----------------------+\n",
      "|5348|  9| 06|  22|    46|                 11|                  14.08|                  33.18|\n",
      "+----+---+---+----+------+-------------------+-----------------------+-----------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_ip_os_minute_clicks.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25199 observations.\n"
     ]
    }
   ],
   "source": [
    "f_app_hour_clicks = spark.read.csv(os.path.join(mungepath, \"f_app_hour_clicks\"), header=True)\n",
    "print('Found %d observations.' %f_app_hour_clicks.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+---------------+-------------------+-------------------+\n",
      "|app|day|hour|app_hour_clicks|app_hour_std_minute|app_hour_avg_minute|\n",
      "+---+---+----+---------------+-------------------+-------------------+\n",
      "|183| 07|  03|            894|               17.5|              37.56|\n",
      "+---+---+----+---------------+-------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_app_hour_clicks.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 53535 observations.\n"
     ]
    }
   ],
   "source": [
    "f_device_hour_clicks = spark.read.csv(os.path.join(mungepath, \"f_device_hour_clicks\"), header=True)\n",
    "print('Found %d observations.' %f_device_hour_clicks.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----+------------------+----------------------+----------------------+\n",
      "|device|day|hour|device_hour_clicks|device_hour_std_minute|device_hour_avg_minute|\n",
      "+------+---+----+------------------+----------------------+----------------------+\n",
      "|   128| 07|  00|                 1|                   0.0|                  11.0|\n",
      "+------+---+----+------------------+----------------------+----------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_device_hour_clicks.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19600 observations.\n"
     ]
    }
   ],
   "source": [
    "f_os_hour_clicks = spark.read.csv(os.path.join(mungepath, \"f_os_hour_clicks\"), header=True)\n",
    "print('Found %d observations.' %f_os_hour_clicks.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+--------------+------------------+------------------+\n",
      "| os|day|hour|os_hour_clicks|os_hour_std_minute|os_hour_avg_minute|\n",
      "+---+---+----+--------------+------------------+------------------+\n",
      "|174| 07|  10|            19|             18.98|             22.42|\n",
      "+---+---+----+--------------+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_os_hour_clicks.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15659 observations.\n"
     ]
    }
   ],
   "source": [
    "f_channel_hour_clicks = spark.read.csv(os.path.join(mungepath, \"f_channel_hour_clicks\"), header=True)\n",
    "print('Found %d observations.' %f_channel_hour_clicks.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----+-------------------+-----------------------+-----------------------+\n",
      "|channel|day|hour|channel_hour_clicks|channel_hour_std_minute|channel_hour_avg_minute|\n",
      "+-------+---+----+-------------------+-----------------------+-----------------------+\n",
      "|    278| 06|  23|              10066|                  17.14|                  31.61|\n",
      "+-------+---+----+-------------------+-----------------------+-----------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_channel_hour_clicks.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+---+-------+-------------------+---------------+-------------+---+----+------+------+\n",
      "|   ip|app|device| os|channel|         click_time|attributed_time|is_attributed|day|hour|minute|second|\n",
      "+-----+---+------+---+-------+-------------------+---------------+-------------+---+----+------+------+\n",
      "|83230|  3|     1| 13|    379|2017-11-06 14:32:21|           null|            0| 06|  14|    32|    21|\n",
      "|17357|  3|     1| 19|    379|2017-11-06 14:33:34|           null|            0| 06|  14|    33|    34|\n",
      "|35810|  3|     1| 13|    379|2017-11-06 14:34:12|           null|            0| 06|  14|    34|    12|\n",
      "+-----+---+------+---+-------+-------------------+---------------+-------------+---+----+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row number as ID variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modeling1 size:  184903890\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "modeling1 = (train\n",
    "             .withColumn('id', monotonically_increasing_id())\n",
    "             .drop('click_time', 'attributed_time'))\n",
    "print(\"modeling1 size: \", modeling1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+---+-------+-------------+---+----+------+------+---+\n",
      "|    ip|app|device| os|channel|is_attributed|day|hour|minute|second| id|\n",
      "+------+---+------+---+-------+-------------+---+----+------+------+---+\n",
      "| 83230|  3|     1| 13|    379|            0| 06|  14|    32|    21|  0|\n",
      "| 17357|  3|     1| 19|    379|            0| 06|  14|    33|    34|  1|\n",
      "| 35810|  3|     1| 13|    379|            0| 06|  14|    34|    12|  2|\n",
      "| 45745| 14|     1| 13|    478|            0| 06|  14|    34|    52|  3|\n",
      "|161007|  3|     1| 13|    379|            0| 06|  14|    35|    08|  4|\n",
      "+------+---+------+---+-------+-------------+---+----+------+------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modeling1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size:  18790469\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "test = (test\n",
    "        .withColumn('id', monotonically_increasing_id())\n",
    "        .drop('click_time'))\n",
    "print(\"test size: \", test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---+------+---+-------+---+----+------+------+---+\n",
      "|click_id|    ip|app|device| os|channel|day|hour|minute|second| id|\n",
      "+--------+------+---+------+---+-------+---+----+------+------+---+\n",
      "|       0|  5744|  9|     1|  3|    107| 10|  04|    00|    00|  0|\n",
      "|       1|119901|  9|     1|  3|    466| 10|  04|    00|    00|  1|\n",
      "|       2| 72287| 21|     1| 19|    128| 10|  04|    00|    00|  2|\n",
      "|       3| 78477| 15|     1| 13|    111| 10|  04|    00|    00|  3|\n",
      "|       4|123080| 12|     1| 13|    328| 10|  04|    00|    00|  4|\n",
      "+--------+------+---+------+---+-------+---+----+------+------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 modeling data set\n",
    "This is the data set that will serve to create the random forest model.  \n",
    "It contains only observations that we scored out of bag with a previous logisitic regression model.  \n",
    "This dataset will be split later into train and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modeling2 size:  92458650\n"
     ]
    }
   ],
   "source": [
    "modeling2 = modeling1.join(f_lr_hash_modeling2.select('id'), on='id', how='inner')\n",
    "print(\"modeling2 size: \", modeling2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+---+------+---+-------+-------------+---+----+------+------+\n",
      "|  id|    ip|app|device| os|channel|is_attributed|day|hour|minute|second|\n",
      "+----+------+---+------+---+-------+-------------+---+----+------+------+\n",
      "|  26| 47902|  3|     1| 17|    379|            0| 06|  14|    48|    07|\n",
      "|1677|191053|  3|     1| 22|    409|            0| 06|  16|    00|    02|\n",
      "|1806|  9587|  3|     1| 53|    480|            0| 06|  16|    00|    02|\n",
      "+----+------+---+------+---+-------+-------------+---+----+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modeling2.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "\n",
    "We are now ready to start working with the actual click data, and our first task involves splitting it into training, validation sets.  Use the [randomSplit method](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit) with the specified weights and seed to create DFs storing each of these datasets, and then [cache](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.cache) each of these DFs, as we will be accessing them multiple times in the remainder of this lab. Finally, compute the size of each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll hold out 10% of our data for validation and leave 90% for training\n",
    "seed = 2210\n",
    "(train_sample, validation) = modeling2.randomSplit([0.9,0.1], seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of positive examples: 205784\n"
     ]
    }
   ],
   "source": [
    "# dataframe of positive examples\n",
    "train_pos = train_sample.filter(col('is_attributed')==1)\n",
    "n_pos = train_pos.count()\n",
    "print(\"number of positive examples:\", n_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative examples: 2057840\n"
     ]
    }
   ],
   "source": [
    "# dataframe of negative examples\n",
    "n_neg = n_pos * 10\n",
    "train_neg = train_sample.filter(col('is_attributed')==0).orderBy(func.rand(seed=seed)).limit(n_neg).cache()\n",
    "print(\"number of negative examples:\", n_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduced training set size: 2263624\n",
      "validation set size: 9246570\n"
     ]
    }
   ],
   "source": [
    "# concat train_pos and train_neg dataframes\n",
    "train_reduced = train_pos.union(train_neg).orderBy(func.rand(seed=seed))\n",
    "\n",
    "# Let's cache these datasets for performance\n",
    "train_reduced.cache()\n",
    "validation = validation.cache()\n",
    "\n",
    "print('reduced training set size:', train_reduced.count())\n",
    "print('validation set size:', validation.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+---+------+---+-------+-------------+---+----+------+------+\n",
      "|          id|    ip|app|device| os|channel|is_attributed|day|hour|minute|second|\n",
      "+------------+------+---+------+---+-------+-------------+---+----+------+------+\n",
      "| 60132345023|155147|  2|     1| 18|    477|            0| 07|  04|    53|    51|\n",
      "|360777256570|  4710|  3|     1| 17|    173|            0| 09|  01|    58|    37|\n",
      "|283470834119| 24506| 18|     1| 17|    134|            0| 08|  13|    01|    32|\n",
      "+------------+------+---+------+---+-------+-------------+---+----+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_reduced.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data binding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bind_features(inputDF, train_test=None):\n",
    "    \n",
    "    if train_test == \"train\":\n",
    "        \n",
    "        # add f_lr_hash_modeling\n",
    "        dataset = inputDF.join(f_lr_hash_modeling2, on='id', how='inner')\n",
    "        \n",
    "        # add f_close_clicks_train\n",
    "        dataset = dataset.join(f_close_clicks_train, on='id', how='inner')\n",
    "        \n",
    "        # add f_close_clicks_app_train\n",
    "        dataset = dataset.join(f_close_clicks_app_train, on='id', how='inner')\n",
    "        \n",
    "        # add f_close_clicks_device_train\n",
    "        dataset = dataset.join(f_close_clicks_device_train, on='id', how='inner')\n",
    "        \n",
    "        # add f_close_clicks_os_train\n",
    "        dataset = dataset.join(f_close_clicks_os_train, on='id', how='inner')\n",
    "    \n",
    "    if train_test == \"test\":\n",
    "        \n",
    "        # add f_lr_hash_test\n",
    "        dataset = inputDF.join(f_lr_hash_test, on='id', how='inner')\n",
    "        \n",
    "        # add f_close_clicks_test\n",
    "        dataset = dataset.join(f_close_clicks_test, on='id', how='inner')\n",
    "        \n",
    "        # add f_close_clicks_app_test\n",
    "        dataset = dataset.join(f_close_clicks_app_test, on='id', how='inner')\n",
    "        \n",
    "        # add f_close_clicks_device_test\n",
    "        dataset = dataset.join(f_close_clicks_device_test, on='id', how='inner')\n",
    "        \n",
    "        # add f_close_clicks_os_test\n",
    "        dataset = dataset.join(f_close_clicks_os_test, on='id', how='inner')\n",
    "        \n",
    "    \n",
    "    # add f_counts_hour_ip\n",
    "    dataset = dataset.join(f_counts_hour_ip, on=['ip', 'day', 'hour'], how='left_outer')\n",
    "    \n",
    "    # add f_counts_minute_ip\n",
    "    dataset = dataset.join(f_counts_minute_ip, on=['ip', 'day', 'hour', 'minute'], how='left_outer')\n",
    "    \n",
    "    # add f_counts_week_ip\n",
    "    dataset = dataset.join(f_counts_week_ip, on=['ip'], how='left_outer')\n",
    "    \n",
    "    # add f_counts_week_app\n",
    "    dataset = dataset.join(f_counts_week_app, on=['app'], how='left_outer')\n",
    "    \n",
    "    # add f_counts_week_device\n",
    "    dataset = dataset.join(f_counts_week_device, on=['device'], how='left_outer')\n",
    "    \n",
    "    # add f_counts_week_os\n",
    "    dataset = dataset.join(f_counts_week_os, on=['os'], how='left_outer')\n",
    "    \n",
    "    # add f_counts_week_channel\n",
    "    dataset = dataset.join(f_counts_week_channel, on=['channel'], how='left_outer')\n",
    "    \n",
    "    # add f_ip_app_clicks\n",
    "    dataset = dataset.join(f_ip_app_clicks, on=['ip', 'app'], how='left_outer')\n",
    "    \n",
    "    # add f_ip_device_clicks\n",
    "    dataset = dataset.join(f_ip_device_clicks, on=['ip', 'device'], how='left_outer')        \n",
    "\n",
    "    # add f_ip_os_clicks\n",
    "    dataset = dataset.join(f_ip_os_clicks, on=['ip', 'os'], how='left_outer')\n",
    "    \n",
    "    # add f_ip_app_hour_clicks\n",
    "    dataset = dataset.join(f_ip_app_hour_clicks, on=['ip', 'app', 'day', 'hour'], how='left_outer')\n",
    "    \n",
    "    # add f_ip_device_hour_clicks\n",
    "    dataset = dataset.join(f_ip_device_hour_clicks, on=['ip', 'device', 'day', 'hour'], how='left_outer')        \n",
    "\n",
    "    # add f_ip_os_hour_clicks\n",
    "    dataset = dataset.join(f_ip_os_hour_clicks, on=['ip', 'os', 'day', 'hour'], how='left_outer')\n",
    "    \n",
    "    # add f_ip_app_minute_clicks\n",
    "    dataset = dataset.join(f_ip_app_minute_clicks, on=['ip', 'app', 'day', 'hour', 'minute'], how='left_outer')\n",
    "    \n",
    "    # add f_ip_device_minute_clicks\n",
    "    dataset = dataset.join(f_ip_device_minute_clicks, on=['ip', 'device', 'day', 'hour', 'minute'], how='left_outer')        \n",
    "\n",
    "    # add f_ip_os_minute_clicks\n",
    "    dataset = dataset.join(f_ip_os_minute_clicks, on=['ip', 'os', 'day', 'hour', 'minute'], how='left_outer')\n",
    "      \n",
    "    # add f_app_hour_clicks\n",
    "    dataset = dataset.join(f_app_hour_clicks, on=['app', 'day', 'hour'], how='left_outer')\n",
    "    \n",
    "    # add f_device_hour_clicks\n",
    "    dataset = dataset.join(f_device_hour_clicks, on=['device', 'day', 'hour'], how='left_outer')\n",
    "    \n",
    "    # add f_os_hour_clicks\n",
    "    dataset = dataset.join(f_os_hour_clicks, on=['os', 'day', 'hour'], how='left_outer')\n",
    "    \n",
    "    # add f_channel_hour_clicks\n",
    "    dataset = dataset.join(f_channel_hour_clicks, on=['channel', 'day', 'hour'], how='left_outer')\n",
    "    \n",
    "    # apply the original chronological order\n",
    "    dataset = dataset.orderBy(col('id'))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2263624"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add features to train_reduced\n",
    "train_features = bind_features(train_reduced, train_test=\"train\").cache()\n",
    "\n",
    "# check wether the join operations where right\n",
    "train_features.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9246570"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add features to validation\n",
    "validation_features = bind_features(validation, train_test=\"train\").cache()\n",
    "\n",
    "# check wether the join operations where right\n",
    "validation_features.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18790469"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add features to test\n",
    "test_features = bind_features(test, train_test=\"test\").cache()\n",
    "\n",
    "# check wether the join operations where right\n",
    "test_features.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training data to disk\n",
    "(train_features\n",
    " .coalesce(1)\n",
    " .write\n",
    " .format(\"com.databricks.spark.csv\")\n",
    " .option(\"header\", \"true\")\n",
    " .mode(\"overwrite\")\n",
    " .save(mungepath+\"model_data/20180504/rf_lr_lasso_inter2_noip/train_features\", compression=\"None\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save validation data to disk\n",
    "(validation_features\n",
    " .coalesce(1)\n",
    " .write\n",
    " .format(\"com.databricks.spark.csv\")\n",
    " .option(\"header\", \"true\")\n",
    " .mode(\"overwrite\")\n",
    " .save(mungepath+\"model_data/20180504/rf_lr_lasso_inter2_noip/validation_features\", compression=\"None\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save test data to disk\n",
    "(test_features\n",
    " .coalesce(1)\n",
    " .write\n",
    " .format(\"com.databricks.spark.csv\")\n",
    " .option(\"header\", \"true\")\n",
    " .mode(\"overwrite\")\n",
    " .save(mungepath+\"model_data/20180504/rf_lr_lasso_inter2_noip/test_features\", compression=\"None\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-computed modeling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in train : 2263624\n"
     ]
    }
   ],
   "source": [
    "train_features = spark.read.csv(os.path.join(mungepath,\"model_data/20180504/rf_lr_lasso_inter2_noip/train_features/*\"), header=True)\n",
    "print(\"Number of observations in train :\", train_features.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in validation : 9246570\n"
     ]
    }
   ],
   "source": [
    "validation_features = spark.read.csv(os.path.join(mungepath,\"model_data/20180504/rf_lr_lasso_inter2_noip/validation_features/*\"), header=True)\n",
    "print(\"Number of observations in validation :\", validation_features.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in test : 18790469\n"
     ]
    }
   ],
   "source": [
    "test_features = spark.read.csv(os.path.join(mungepath,\"model_data/20180504/rf_lr_lasso_inter2_noip/test_features/*\"), header=True)\n",
    "print(\"Number of observations in test :\", test_features.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(train_features.limit(1).toPandas().columns)\n",
    "feature_names = [f for f in feature_names if f not in ['ip','os','device','app','channel',\n",
    "                                                       'day',\n",
    "                                                       'id','is_attributed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features: 109\n"
     ]
    }
   ],
   "source": [
    "print(\"number of features:\", len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hour',\n",
       " 'minute',\n",
       " 'second',\n",
       " 'f_lr_hash_inter2_2p18_noip',\n",
       " 'lapse_prev_click',\n",
       " 'lapse_next_click',\n",
       " 'lapse_prev_click_app',\n",
       " 'lapse_next_click_app',\n",
       " 'lapse_prev_click_device',\n",
       " 'lapse_next_click_device',\n",
       " 'lapse_prev_click_os',\n",
       " 'lapse_next_click_os',\n",
       " 'ip_hour_nb_clicks',\n",
       " 'ip_hour_nb_apps',\n",
       " 'ip_hour_nb_devices',\n",
       " 'ip_hour_nb_os',\n",
       " 'ip_hour_nb_channels',\n",
       " 'ip_hour_std_minute',\n",
       " 'ip_hour_avg_minute',\n",
       " 'ip_minute_nb_clicks',\n",
       " 'ip_minute_nb_apps',\n",
       " 'ip_minute_nb_devices',\n",
       " 'ip_minute_nb_os',\n",
       " 'ip_minute_nb_channels',\n",
       " 'ip_minute_std_second',\n",
       " 'ip_minute_avg_second',\n",
       " 'ip_nb_clicks',\n",
       " 'ip_nb_apps',\n",
       " 'ip_nb_devices',\n",
       " 'ip_nb_os',\n",
       " 'ip_nb_channels',\n",
       " 'ip_std_click_time',\n",
       " 'ip_std_hour',\n",
       " 'ip_std_minute',\n",
       " 'ip_avg_hour',\n",
       " 'ip_avg_minute',\n",
       " 'app_nb_clicks',\n",
       " 'app_nb_ips',\n",
       " 'app_nb_devices',\n",
       " 'app_nb_os',\n",
       " 'app_nb_channels',\n",
       " 'app_std_click_time',\n",
       " 'app_std_hour',\n",
       " 'app_std_minute',\n",
       " 'app_avg_hour',\n",
       " 'app_avg_minute',\n",
       " 'device_nb_clicks',\n",
       " 'device_nb_apps',\n",
       " 'device_nb_ips',\n",
       " 'device_nb_os',\n",
       " 'device_nb_channels',\n",
       " 'os_nb_clicks',\n",
       " 'os_nb_apps',\n",
       " 'os_nb_devices',\n",
       " 'os_nb_ips',\n",
       " 'os_nb_channels',\n",
       " 'channel_nb_clicks',\n",
       " 'channel_nb_apps',\n",
       " 'channel_nb_devices',\n",
       " 'channel_nb_os',\n",
       " 'channel_nb_ips',\n",
       " 'ip_app_clicks',\n",
       " 'ip_app_std_click_time',\n",
       " 'ip_app_std_hour',\n",
       " 'ip_app_std_minute',\n",
       " 'ip_app_avg_hour',\n",
       " 'ip_app_avg_minute',\n",
       " 'ip_device_clicks',\n",
       " 'ip_device_std_click_time',\n",
       " 'ip_device_std_hour',\n",
       " 'ip_device_std_minute',\n",
       " 'ip_device_avg_hour',\n",
       " 'ip_device_avg_minute',\n",
       " 'ip_os_clicks',\n",
       " 'ip_os_std_click_time',\n",
       " 'ip_os_std_hour',\n",
       " 'ip_os_std_minute',\n",
       " 'ip_os_avg_hour',\n",
       " 'ip_os_avg_minute',\n",
       " 'ip_app_hour_clicks',\n",
       " 'ip_app_hour_std_minute',\n",
       " 'ip_app_hour_avg_minute',\n",
       " 'ip_device_hour_clicks',\n",
       " 'ip_device_hour_std_minute',\n",
       " 'ip_device_hour_avg_minute',\n",
       " 'ip_os_hour_clicks',\n",
       " 'ip_os_hour_std_minute',\n",
       " 'ip_os_hour_avg_minute',\n",
       " 'ip_app_minute_clicks',\n",
       " 'ip_app_minute_std_second',\n",
       " 'ip_app_minute_avg_second',\n",
       " 'ip_device_minute_clicks',\n",
       " 'ip_device_minute_std_second',\n",
       " 'ip_device_minute_avg_second',\n",
       " 'ip_os_minute_clicks',\n",
       " 'ip_os_minute_std_second',\n",
       " 'ip_os_minute_avg_second',\n",
       " 'app_hour_clicks',\n",
       " 'app_hour_std_minute',\n",
       " 'app_hour_avg_minute',\n",
       " 'device_hour_clicks',\n",
       " 'device_hour_std_minute',\n",
       " 'device_hour_avg_minute',\n",
       " 'os_hour_clicks',\n",
       " 'os_hour_std_minute',\n",
       " 'os_hour_avg_minute',\n",
       " 'channel_hour_clicks',\n",
       " 'channel_hour_std_minute',\n",
       " 'channel_hour_avg_minute']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assembling features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change column types\n",
    "train_ml = train_features.withColumn('label', col('is_attributed').cast('integer'))\n",
    "validation_ml = validation_features.withColumn('label', col('is_attributed').cast('integer'))\n",
    "\n",
    "for f in feature_names:\n",
    "    train_ml = train_ml.withColumn(f, col(f).cast('float'))\n",
    "    validation_ml = validation_ml.withColumn(f, col(f).cast('float'))\n",
    "\n",
    "# keep only target and feature variable\n",
    "train_ml = train_ml.select(['id', 'label']+feature_names)\n",
    "validation_ml = validation_ml.select(['id', 'label']+feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in building our ML pipeline is to convert the predictor features from DataFrame columns to Feature Vectors using the pyspark.ml.feature.VectorAssembler() method.\n",
    "\n",
    "The VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees. VectorAssembler takes a list of input column names (each is a string) and the name of the output column (as a string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorAssembler_44db84b530b572911319"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorizer = VectorAssembler()\n",
    "vectorizer.setInputCols(feature_names)\n",
    "vectorizer.setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+\n",
      "| id|label|            features|\n",
      "+---+-----+--------------------+\n",
      "|  1|    0|[14.0,33.0,34.0,3...|\n",
      "| 53|    0|[15.0,5.0,5.0,3.3...|\n",
      "|153|    0|[15.0,42.0,55.0,3...|\n",
      "|186|    0|[15.0,45.0,5.0,9....|\n",
      "|223|    0|[15.0,46.0,45.0,3...|\n",
      "+---+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_ml = (vectorizer\n",
    "            .transform(train_ml)\n",
    "            .select('id', 'label', 'features')\n",
    "            .cache())\n",
    "validation_ml = (vectorizer\n",
    "                 .transform(validation_ml)\n",
    "                 .select('id', 'label', 'features')\n",
    "                 .cache())\n",
    "\n",
    "train_ml.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxBins: 32 \tnumTrees: 100 \tmaxDepth: 9 \tauc_train: 0.971886 \tauc_val: 0.972128\n",
      "maxBins: 32 \tnumTrees: 100 \tmaxDepth: 11 \tauc_train: 0.975454 \tauc_val: 0.97482\n",
      "maxBins: 32 \tnumTrees: 100 \tmaxDepth: 13 \tauc_train: 0.979628 \tauc_val: 0.976586\n",
      "maxBins: 32 \tnumTrees: 100 \tmaxDepth: 15 \tauc_train: 0.984863 \tauc_val: 0.977687\n",
      "maxBins: 32 \tnumTrees: 300 \tmaxDepth: 9 \tauc_train: 0.971739 \tauc_val: 0.971915\n",
      "maxBins: 32 \tnumTrees: 300 \tmaxDepth: 11 \tauc_train: 0.975458 \tauc_val: 0.974685\n",
      "maxBins: 32 \tnumTrees: 300 \tmaxDepth: 13 \tauc_train: 0.979892 \tauc_val: 0.976621\n",
      "maxBins: 32 \tnumTrees: 300 \tmaxDepth: 15 \tauc_train: 0.985273 \tauc_val: 0.97779\n",
      "maxBins: 32 \tnumTrees: 500 \tmaxDepth: 9 \tauc_train: 0.971876 \tauc_val: 0.97208\n",
      "maxBins: 32 \tnumTrees: 500 \tmaxDepth: 11 \tauc_train: 0.975549 \tauc_val: 0.974782\n",
      "maxBins: 32 \tnumTrees: 500 \tmaxDepth: 13 \tauc_train: 0.979954 \tauc_val: 0.976672\n",
      "maxBins: 32 \tnumTrees: 500 \tmaxDepth: 15 \tauc_train: 0.985402 \tauc_val: 0.977909\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# tuning grid\n",
    "grid_maxBins = [32]\n",
    "grid_numTrees = [100, 300, 500]\n",
    "grid_maxDepth = [9, 11, 13, 15]\n",
    "\n",
    "# Create an AUC ROC evaluator using the label and predicted columns\n",
    "bcEval = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "\n",
    "# initialize storage for cross validation results and models\n",
    "cross_validation = pd.DataFrame(columns=['maxBins', 'numTrees', 'maxDepth', 'auc_train', 'auc_val'])\n",
    "model_dict = dict()\n",
    "\n",
    "# loop over tuning parameters\n",
    "for mb in grid_maxBins:\n",
    "    for nt in grid_numTrees:\n",
    "        for md in grid_maxDepth:\n",
    "            rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed=22,\n",
    "                                        maxDepth=md, numTrees=nt, maxBins=mb)\n",
    "            model = rf.fit(train_ml)\n",
    "            model_dict[str((mb, nt, md))] = model                                     \n",
    "            auc_train = round(bcEval.evaluate(model.transform(train_ml)),6)\n",
    "            auc_val = round(bcEval.evaluate(model.transform(validation_ml)),6)\n",
    "            cross_validation = cross_validation.append({'maxBins':mb,\n",
    "                                                        'maxDepth':md,\n",
    "                                                        'numTrees':nt,\n",
    "                                                        'auc_train':auc_train,\n",
    "                                                        'auc_val':auc_val},\n",
    "                                                       ignore_index=True)\n",
    "            print('maxBins:', mb, '\\tnumTrees:', nt, '\\tmaxDepth:', md, '\\tauc_train:', auc_train, '\\tauc_val:', auc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python3",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
